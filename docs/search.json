[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html",
    "href": "posts/03-confidence_intervals/i.html",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "",
    "text": "small: n = 4\nmedium: n = 20\nlarge: n = 100\np (close): p = 0.46\np (far): p = 0.87"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#step-1",
    "href": "posts/03-confidence_intervals/i.html#step-1",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "",
    "text": "small: n = 4\nmedium: n = 20\nlarge: n = 100\np (close): p = 0.46\np (far): p = 0.87"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#function-for-generating-sample-proportion",
    "href": "posts/03-confidence_intervals/i.html#function-for-generating-sample-proportion",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Function for generating sample proportion",
    "text": "Function for generating sample proportion\n\nlibrary(resample)\nlibrary(tidyverse)\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # 90% confidence interval\n  lb &lt;- phat - qnorm(0.95) * (sqrt(phat * (1 - phat) / n))\n  ub &lt;- phat + qnorm(0.95) * (sqrt(phat * (1 - phat) / n))\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  \n  return (prop_df)\n}"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#n-100-p-0.87",
    "href": "posts/03-confidence_intervals/i.html#n-100-p-0.87",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "n = 100, p = 0.87",
    "text": "n = 100, p = 0.87\n\\(np = 100 * 0.87 = 87\\)\n\\(n(1-p) = 100 * (1-0.87) = 13\\)\nThe large sample assumption holds because both checks are greater than 10!\n\n## n = 100, p = 0.87\nn_sim &lt;- 5000\np &lt;- 0.87\n\nprop_ci_df &lt;- map(1:n_sim, \\(i) generate_samp_prop(n=100, p=0.87)) |&gt; bind_rows()\n\nmany_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.109         0.882"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#n-100-p-0.46",
    "href": "posts/03-confidence_intervals/i.html#n-100-p-0.46",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "n = 100, p = 0.46",
    "text": "n = 100, p = 0.46\n\\(np = 100 * 0.46 = 46\\)\n\\(n(1-p) = 100 * (1-0.46) = 54\\)\nThe large sample assumption holds because both checks are greater than 10!\n\n## n = 100, p = 0.46\nn_sim &lt;- 5000\np &lt;- 0.46\n\nprop_ci_df &lt;- map(1:n_sim, \\(i) generate_samp_prop(n=100, p=0.46)) |&gt; bind_rows()\n\nmany_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.163         0.895"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#n-20-p-0.87",
    "href": "posts/03-confidence_intervals/i.html#n-20-p-0.87",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "n = 20, p = 0.87",
    "text": "n = 20, p = 0.87\n\\(np = 20 * 0.87 = 17.4\\)\n\\(n(1-p) = 20 * (1-0.87) = 2.6\\)\nThe large sample size assumption check does not hold because n(1-p) is not greater than 10.\n\n## n = 20, p = 0.87\nn_sim &lt;- 5000\np &lt;- 0.87\n\nprop_ci_df &lt;- map(1:n_sim, \\(i) generate_samp_prop(n=20, p=0.87)) |&gt; bind_rows()\n\nmany_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.228         0.907"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#n-20-p-0.46",
    "href": "posts/03-confidence_intervals/i.html#n-20-p-0.46",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "n = 20, p = 0.46",
    "text": "n = 20, p = 0.46\n\\(np = 20 * 0.46 = 9.2\\)\n\\(n(1-p) = 20 * (1-0.46) = 10.8\\)\nThe large sample size assumption check does not hold because np is not greater than 10.\n\n## n = 20, p = 0.46\nn_sim &lt;- 5000\np &lt;- 0.46\n\nprop_ci_df &lt;- map(1:n_sim, \\(i) generate_samp_prop(n=20, p=0.46)) |&gt; bind_rows()\n\nmany_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.357         0.884"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#n-4-p-0.87",
    "href": "posts/03-confidence_intervals/i.html#n-4-p-0.87",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "n = 4, p = 0.87",
    "text": "n = 4, p = 0.87\n\\(np = 4 * 0.87 = 3.48\\)\n\\(n(1-p) = 4 * (1-0.87) = 0.52\\)\nThe large sample size assumption check does not hold because both checks are less than 10.\n\n## n = 4, p = 0.87\nn_sim &lt;- 5000\np &lt;- 0.87\n\nprop_ci_df &lt;- map(1:n_sim, \\(i) generate_samp_prop(n=4, p=0.87)) |&gt; bind_rows()\n\nmany_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.316         0.426"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#n-4-p-0.46",
    "href": "posts/03-confidence_intervals/i.html#n-4-p-0.46",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "n = 4, p = 0.46",
    "text": "n = 4, p = 0.46\n\\(np = 4 * 0.46 = 1.84\\)\n\\(n(1-p) = 4 * (1-0.46) = 2.16\\)\nThe large sample size assumption check does not hold because both checks are less than 10.\n\n## n = 4, p = 0.46\nn_sim &lt;- 5000\np &lt;- 0.46\n\nprop_ci_df &lt;- map(1:n_sim, \\(i) generate_samp_prop(n=4, p=0.46)) |&gt; bind_rows()\n\nmany_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.663         0.874"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#results",
    "href": "posts/03-confidence_intervals/i.html#results",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Results",
    "text": "Results\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 4\\)\n\\(n = 20\\)\n\\(n = 100\\)\n\n\n\n\n\\(p = 0.46\\)\nCoverage Rate\n0.872\n0.890\n0.885\n\n\n\\(p = 0.87\\)\nCoverage Rate\n0.415\n0.897\n0.878\n\n\n\n\n\n\n\n\n\n\\(p = 0.46\\)\nAverage Width\n0.662\n0.357\n0.163\n\n\n\\(p = 0.87\\)\nAverage Width\n0.308\n0.227\n0.109"
  },
  {
    "objectID": "posts/03-confidence_intervals/i.html#summary-of-results",
    "href": "posts/03-confidence_intervals/i.html#summary-of-results",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Summary of Results",
    "text": "Summary of Results\nFor the case where n = 4 and p = 0.87, the coverage rate is quite low at 0.415, meaning the true value of p falls within the confidence interval in less than half of the simulations. In contrast, for n = 20 and p = 0.87, the coverage rate is significantly higher at 0.897, indicating that the confidence interval captures the true value in almost 90% of simulations, which is what we want with a 90% confidence level. Among all cases, this scenario has the highest coverage rate. The n = 100 cases are also quite good with a coverage rates close to 0.9. Additionally, the variation in coverage rate is much greater for small sample sizes (range of 0.457) compared to the larger sample sizes, which both only vary by 0.07.\nAs sample size increases, the average interval width decreases. For instance, when n = 4, p = 0.46, the interval width is quite large at 0.662, but when n = 100, p = 0.87, it shrinks to 0.109. The difference in average interval width for different values of p is much larger for the small sample size of n = 4 (difference of 0.354) than for the larger sample size of n = 100 (difference of 0.054). Across all sample sizes, p = 0.87 consistently results in a smaller average interval width.\nAdditionally, both n = 4 and n = 20 violate the necessary sample size assumptions, whereas n = 100 is large enough to meet these conditions. The results for n = 100 align with what we expect from a simulated confidence interval, whereas the smaller sample sizes show more skewed outcomes. Interestingly, for n = 4, p = 0.46, the coverage rate is relatively high at 0.872, which can be attributed to the large average interval width."
  },
  {
    "objectID": "posts/05-hypothesis_testing/hypothesistesting.html",
    "href": "posts/05-hypothesis_testing/hypothesistesting.html",
    "title": "Mini Project 5: Advantages and Drawbacks of Using p-values",
    "section": "",
    "text": "1. Towards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\n\nI think what the authors mean is that we’ll stop just chasing p &lt; 0.05 and instead actually think about what the data is saying in the bigger picture.\nStatistical thinking is more about asking: does this result make sense? Is it useful? Are there other factors at play? A thoughtful statistician should take these questions into account when reporting their findings.\nIt’s moving away from a “did I pass or fail?” mindset and towards understanding and interpreting results like a scientist would. This is kind of like moving away from determining tennis success by match results towards reflecting on how I competed / performed.\n\n\n2. Section 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\n\nBasically, calling something “statistically significant” is just restating the p-value in a more dramatic way — and usually oversimplifying it.\nThis antiquated label makes it seem like there’s a huge difference between a result just above and just below 0.05, which there typically isn’t.\nIt creates a black-and-white interpretation of something that really lives on a gradient/spectrum. For example, p = 0.049 vs p = 0.051 shows up as two different things, when they really aren’t that different at all.\nThis can be difficult for people who aren’t experienced in stats but are trying to use stats to back up their findings. In my experience, I’ve found this to be pretty common in the hard sciences since I’ve taken many biology and chemistry classes.\n\n\n3. Section 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\n\nI 100% agree — the idea that we should only talk about results that hit some arbitrary cutoff doesn’t feel scientific to me.\n\nThis reminds me of when I helped a Senior on the tennis team a couple years ago with conducting a t-test for their research project. All he cared about was hunting down “statistically significant” p-value to slap into his paper and didn’t want to include the result if the p-value wasn’t good enough. 🦞\n\nI tried to explain to him that sometimes a result with p = 0.09 could still be super interesting or support a bigger story in his data — a result like this shouldn’t be ignored just because they missed the mark.\nWhat we choose to present should depend on how meaningful, relevant, or surprising the result is — not whether it passes some statistical checkpoint.\n\n\n4. Section 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\n\nTotally agree with this one too. Not every dataset or research question should be treated the same since data comes in so many different forms and should require different scales of assessing significance.\nSome studies might need formal tests (like in medical sciences that require high amounts of precision), others are more exploratory (like an ecological survey), and some are about estimation or prediction (like trying to predict a stock price, which requires a high degree of flexibility).\nTrying to force everyone into the same box (like p-values or NHST) kind of defeats the purpose of using stats thoughtfully in the first place.\n\n\n5. Section 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\n\nTo me, “statistical thoughtfulness” is caring about what your analysis means and drawing meaningful conclusions instead of just caring about the numbers.\nIt’s not about doing a fancy test — it’s about thinking critically about assumptions, interpreting your results carefully, and communicating them honestly. I feel like there is no room for dishonestly in the scientific world since science is the backbone that is holding our fragile existence on Earth together.\nA thoughtful analysis shows that the person behind it really understood the data and took the time to be clear, even if it’s a messy, complicated, or hard-to-present result.\n\n\n6. Section 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\n\nI think the problem is that words like “significance” and “confidence” sound way more certain and conclusive than they actually are. You still have to think about why you’re getting a significant result and what are the inner-workings behind the result.\n“Compatibility” feels like a softer, more accurate word — it tells you that a result fits with the data, but doesn’t claim anything absolute. It’s by no means a perfect word, but it might be less misleading.\nI do think switching to better language along these lines could go a long way in helping people (even researchers) avoid misinterpreting what their results actually mean.\n\n\n7. Find a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?\n\nInteresting quote: “As Gelman and Stern (2006) famously observed, the difference between ‘significant’ and ‘not significant’ is not itself statistically significant.” (Section 2, last sentence of paragraph 4).\nThis quote gave me shivers when I read it because I’ve always thought (through my long 4 years of being a statistician) how silly it is to draw a hard line at p = 0.05, one side being good and the other side being bad.\nThe difference between a p = 0.049 and p = 0.051 is next to none, and using the p = 0.05 cutoff incorrectly dichotomizes the two results. That’s why cutoffs in statistics should be a thing of the past, and it should become more of a gradient."
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html",
    "href": "posts/04-bayesian_statistics/bayes.html",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#introduction",
    "href": "posts/04-bayesian_statistics/bayes.html#introduction",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this Mini-Project, I we are doing a Bayesian analysis on the probability that Rafael Nadal wins a point on his own serve against his primary rival, Novak Djokovic, at the French Open. We’re going to attempt to create a Bayesian model with an informative prior distribution updated with actual data from the 2020 French Open meeting between these two players. The resulting posterior distribution should give us some insight into the probability that Nadal wins a point on his serve against Djokovic at the French Open."
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#creating-the-priors",
    "href": "posts/04-bayesian_statistics/bayes.html#creating-the-priors",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Creating the Priors",
    "text": "Creating the Priors\nPrior 1: uniform uninformative prior assumes no prior knowledge and estimates a 50% probability of winning his serve.\n\n# Prior 1: uniformative prior Beta(1,1)\nalpha1 = 1\nbeta1 = 1\n\nPrior 2: informative prior constructed with data from the previous year where Nadal won 46 points on his serve and lost 20 points on his serve. I will assign alpha and beta to these respective win counts. To make sure these values work with the conditions of the problem, I calculated the variance and it is very close to what’s given (SE = 0.05657). We must assume both players’ games haven’t changed in the year since they played last, assume same conditions of play, assume no different momentum from their previous matches, and assume both players are healthy.\n\n# Prior 2: Beta(46,20)\nalpha2 = 46 # successes\nbeta2 = 20 # failures\npossible_params2 = tibble(alpha2, beta2, alpha2/(alpha2 + beta2))\n\npossible_params2 |&gt;\n  mutate(variance = alpha2 * beta2 / ((alpha2 + beta2)^2 * (alpha2 + beta2 + 1))) |&gt;\n  mutate(dist_to_target = abs(variance - 0.05657^2)) # variance is very close\n\n# A tibble: 1 × 5\n  alpha2 beta2 `alpha2/(alpha2 + beta2)` variance dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;                     &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n1     46    20                     0.697  0.00315      0.0000479\n\n\nPrior 3: informative prior based on sports announcer’s estimate of 75% probability and not less than 70%. We will set the constraint on alphas and betas based on the announcer’s estimate and then see which combination of alpha and beta gets us the smallest percentage that goes smaller than 70% probability. First of all, we must assume the sports announcer is a credible and accurate source, assume both players are fully healthy, assume no momentum from their previous matches, and assume normal playing conditions.\n\n# Prior 3: Beta(100,33.3)\nalphas &lt;- seq(0.01, 100, length.out = 2000) \nbetas &lt;- alphas / 3 # derived from 0.75 estimate\n\ntarget_prob &lt;- 0.02\nprob_less_70 &lt;- pbeta(0.70, alphas, betas)\n\ntibble(alphas, betas, alphas/betas, prob_less_70) |&gt;\n  mutate(close_to_target = abs(prob_less_70 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target)) \n\n# A tibble: 1 × 5\n  alphas betas `alphas/betas` prob_less_70 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1    100  33.3              3       0.0946          0.0746"
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#plot-of-the-priors",
    "href": "posts/04-bayesian_statistics/bayes.html#plot-of-the-priors",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Plot of the Priors",
    "text": "Plot of the Priors\n\nps &lt;- seq(0, 1, length.out = 1000)\none_prior &lt;- dbeta(ps, 1, 1)\ntwo_prior &lt;- dbeta(ps, 46, 20)\nthree_prior &lt;- dbeta(ps, 100, 33.3)\n\nplot1 &lt;- tibble(ps, one_prior, two_prior, three_prior) |&gt;\n  pivot_longer(2:4, names_to = \"dist_type\", values_to = \"density\")\n\nggplot(data = plot1, aes(x = ps, y = density, colour = dist_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")"
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#data",
    "href": "posts/04-bayesian_statistics/bayes.html#data",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Data",
    "text": "Data\nWe want to use the 2020 French Open data to update our prior for the probability that Nadal wins a point on serve. In that tournament, the two players played in the final. In that final, Nadal served 84 points and won 56 of those points."
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#posteriors",
    "href": "posts/04-bayesian_statistics/bayes.html#posteriors",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Posteriors",
    "text": "Posteriors\n\\(g(\\theta | y_{obs}) \\sim Beta(\\alpha_{post} = \\alpha_{pre}+56, \\beta_{post} = \\beta_{pre} + 28)\\)\n\n# Posterior 1\napost1 = 1+56\nbpost1 = 1+28\none_post = dbeta(ps, apost1, bpost1)\nmean1 = apost1 / (apost1 + bpost1)\nmean1 # posterior mean with the uninformative prior Beta(1,1)\n\n[1] 0.6627907\n\nqbeta(c(0.05, 0.95), apost1, bpost1) # 90% credible interval\n\n[1] 0.5772453 0.7440061\n\n# Posterior 2\napost2 = 46+56\nbpost2 = 20+28\ntwo_post = dbeta(ps, apost2, bpost2)\nmean2 = apost2 / (apost2 + bpost2)\nmean2 # posterior mean with the informative prior Beta(46,20)\n\n[1] 0.68\n\nqbeta(c(0.05, 0.95), apost2, bpost2) # 90% credible interval\n\n[1] 0.6161904 0.7410715\n\n# Posterior 3\napost3 = 100+56\nbpost3 = 33.3+28\nthree_post = dbeta(ps, apost3, bpost3)\nmean3 = apost3 / (apost3 + bpost3)\nmean3 # posterior mean with the informative prior Beta(100,33.3)\n\n[1] 0.7179015\n\nqbeta(c(0.05, 0.95), apost3, bpost3) # 90% credible interval\n\n[1] 0.6666686 0.7668490"
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#plot-of-the-posteriors",
    "href": "posts/04-bayesian_statistics/bayes.html#plot-of-the-posteriors",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Plot of the Posteriors",
    "text": "Plot of the Posteriors\n\nplot2 &lt;- tibble(ps, one_post, two_post, three_post) |&gt;\n  pivot_longer(2:4, names_to = \"dist_type\", values_to = \"density\")\n\nggplot(data = plot2, aes(x = ps, y = density, colour = dist_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")"
  },
  {
    "objectID": "posts/04-bayesian_statistics/bayes.html#conclusions",
    "href": "posts/04-bayesian_statistics/bayes.html#conclusions",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nEach of the posterior distributions are slightly different because they each come from different prior distributions. The first posterior distribution comes from the uninformative prior, so the data from 2020 is having a significant influence on the posterior distribution, dragging it towards the proportion of \\(56/84 = 0.667\\). The second posterior distribution comes from the informative prior from last year’s data, so I think it might provide the most accurate posterior distribution. Nadal had a higher win percentage on his serve in 2019 than in 2020, so the data update drags the center of the distribution slightly to the left. The third posterior distribution has the highest estimated win probability at 0.72, this is because the announcer’s estimate was high at 0.75.\nThe variance of the third posterior distribution has the lowest of the three (it has the narrowest 90% credible interval). This may be because it has the largest values of \\(\\alpha\\) and \\(\\beta\\), and with the quadratic in the denominator of the variance of a Beta distribution, the resulting variance is small compared to the other posteriors with smaller values for \\(\\alpha\\) and \\(\\beta\\).\nIf I had to choose one distribution to pick above the other two, I would pick the second one because the prior is derived from more accurate information (in my opinion) than the third posterior distribution even though the second posterior has a slightly wider credible interval. Therefore, I can say, based on the 90% credible interval from the second posterior, that there is a 0.9 probability that Nadal’s win percentage while he’s serving against Djokovic at the French Open is between 61.6% and 74.1%.\nIn summary, all three posterior distributions are pretty similar to one another, with minor differences setting the posteriors with informative priors apart. However, even with a uniform prior and only one year of data, the first posterior provides a result that is pretty consistent with the posteriors with informative priors. Additionally, this project effectively shows that for the Beta distribution, larger values of \\(\\alpha\\) and \\(\\beta\\) mean that the model is more credible (has a smaller credible interval and smaller variance). Thus, if there is a situation where you aren’t overly confident in your prior, it’s best to pick smaller values for \\(\\alpha\\) and \\(\\beta\\)."
  },
  {
    "objectID": "posts/06-reflection/reflection.html",
    "href": "posts/06-reflection/reflection.html",
    "title": "Reflection",
    "section": "",
    "text": "Over the course of these five mini-projects throughout this semester in math stat, I’ve truly honed and deepened my understanding of statistical reasoning – not just because of their inherent purpose of practicing and exemplifying what we’ve been learning in class but also because of how they built on one another. Looking back through these projects made me realize how they all work together to paint a rich picture of statistical inference and the under-workings behind dealing with uncertainty. In the moment when I was completing these mini-projects, however, they felt like isolated tasks. The purpose of this reflection essay is to explain some of the key connections that can be drawn between the mini-projects and how they contribute to my understanding of the class as a whole. I think that these mini-projects were designed very well to flow almost seamlessly from theory and simulation to modeling, analysis, and reflection.\nStarting with mini-project 1 – a brief look into sampling distributions of the sample minimum and maximum, it was a great project to highlight the importance (and relative accuracy) of statistical simulation. It was valuable to work through substantial examples of simulating a sampling distribution of the sample min and max for four different probability distributions (Normal, Uniform, Exponential, and Beta), and in the end validate the results of the simulation to the actual theoretical values. This utilization of statistical simulation appeared several more times throughout the semester including mini-project 3 where we used simulation to examine the average interval width and coverage rates of confidence intervals using a binomial distribution. My key takeaway here: don’t underestimate the power of simulation.\nWhen it comes to mini-project 2 (my short story about estimation in the game of tennis), I talked a lot about how tennis players constantly make informed estimations of where the ball is going so that they can be there ready to hit it back. Not only does the content and topic of tennis align directly with what we did for mini-project 4, but the Bayesian analysis we did on tennis data also takes into account using information to improve our models. It’s important to realize that statistics isn’t just theory – one of the main reasons why I like stats so much more than just straight math – it allows you to directly apply any prior knowledge you might have about some data to draw meaningful conclusions. The Bayesian framework was eye-opening for me when thinking about the application of estimation in the frequentist framework (which honestly feels a bit clunky with too many assumptions). The applicable nature of statistics – particularly the flexible nature of Bayesian statistics – has so many upsides, but it also comes with its drawbacks. Since it’s so applicable, so many different fields of science and research use statistics to provide numeric support for the work they’re doing. As such, there is so much naive use of statistics out there which is where mini-project 5 comes in.\nThe reading about p-values was a meaningful read for me because it hit pretty close to home in terms of what it was trying to convey. I’ve always felt like people just abuse statistics to get what they want – like a publication in a science journal or getting a good grade on a research paper for biology class. The idea of statistical significance has always been something that I’ve talked about through my years of education and primarily growing up as a science person. This year has been very insightful to me in terms of finding other ways to do statistical or data science analyses without having to report a p-value with a strict binary cutoff point. I’ve learned that you can obtain meaningful answers to real-world questions without using the traditional frequentist statistician’s formula in the Bayesian tennis analysis. As a whole, reflecting on mini-project 5 and the associated reading has made me realize that my understanding of statistical significance has really matured over time. Mini-project 4 (Bayesian analysis) is just another example of a more nuanced and “colorful” alternative to dichotomous p-value thinking. Going off script a bit here, but my SYE work with visualizing tennis data was instrumental in this deeper and more “colorful” understanding of what statistics and data can be used for.\nAt the end of the day, we’re all just trying to tell a story and it’s nice to know that there are so many different ways to go about doing that with statistics and data in general. Whether it’s trying to examine the accuracy of simulation and visualize sampling distributions (mini-project 1), trying to tell a “meaningful story” about the presence of estimators and estimation in the real world (mini-project 2), investigating the effect of sample size on the accuracy of a distribution of confidence intervals – which can be applied to so many studies out there (mini-project 3), attempting to model the probability of a tennis player winning a point based off of prior knowledge about the players’ tendencies (mini-project 4), or convincing somebody that p-values and significance levels are an antiquated and detrimental way of using statistics in the world of research (mini-project 5), statistics holds so much power in anything that we do. So, having a solid understanding of the inner workings of stats (which is what I’ve gotten from this course and over the course of all the classes I’ve taken at SLU), is going to be instrumental for me going forward. I suppose what I’m trying to get at is these mini-projects in math stat have shown me how interconnected everything in statistics actually is, which is honestly somewhat different from what I felt going through them one at a time. Upon reflection, when I began the semester, I saw mathematical statistics mostly as just an extension of the theory we developed in probability (which it is…), but now I also see it as way of thinking."
  },
  {
    "objectID": "posts/01-sampling_distributions/index.html",
    "href": "posts/01-sampling_distributions/index.html",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "",
    "text": "library(tidyverse)\n## create population graphs\n\nnorm_df &lt;- tibble(x = seq(3, 17, length.out = 1000),\n                  dens = dnorm(x, mean = 10, sd = 2),\n                  pop = \"normal(10, 4)\")\nunif_df &lt;- tibble(x = seq(7, 13, length.out = 1000),\n                  dens = dunif(x, 7, 13),\n                  pop = \"uniform(7, 13)\")\nexp_df &lt;- tibble(x = seq(0, 10, length.out = 1000),\n                 dens = dexp(x, 0.5),\n                 pop = \"exp(0.5)\")\nbeta_df &lt;- tibble(x = seq(0, 1, length.out = 1000),\n                  dens = dbeta(x, 8, 2),\n                  pop = \"beta(8, 2)\")\n\npop_plot &lt;- bind_rows(norm_df, unif_df, exp_df, beta_df) |&gt;\n  mutate(pop = fct_relevel(pop, c(\"normal(10, 4)\", \"uniform(7, 13)\",\n                                  \"exp(0.5)\", \"beta(8, 2)\")))\n\n\n## SIMULATION CODE\n\nnsim &lt;- 5000 # number of simulations\nn = 5 # sample size\n\n## Normal Sampling Distribution of the Sample Minimum\nmu &lt;- 10\nsigma &lt;- 2\n\ngenerate_samp_min_norm &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnorm_mins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_norm(mu = mu, sigma = sigma, n = n))\n\nnorm_mins_df &lt;- tibble(norm_mins)\n\n\nggplot(data = norm_mins_df, aes(x = norm_mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\n\n\nnorm_mins_df |&gt;\n  summarise(mean_samp_dist = mean(norm_mins),\n            var_samp_dist = var(norm_mins),\n            sd_samp_dist = sd(norm_mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           7.70          1.77         1.33\n\n## Normal Sampling Distribution of the Sample Maximum\n\ngenerate_samp_max_norm &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnorm_maxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_norm(mu = mu, sigma = sigma, n = n))\n\nnorm_maxs_df &lt;- tibble(norm_maxs)\n\nggplot(data = norm_maxs_df, aes(x = norm_maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nnorm_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(norm_maxs),\n            var_samp_dist = var(norm_maxs),\n            sd_samp_dist = sd(norm_maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           12.3          1.78         1.33\n\n## Uniform Sampling Distribution of the Sample Minimum\ntheta1 = 7\ntheta2 = 13\n\ngenerate_samp_min_unif &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nunif_mins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_unif(theta1 = theta1, theta2 = theta2, n = n))\n\nunif_mins_df &lt;- tibble(unif_mins)\n\nggplot(data = unif_mins_df, aes(x = unif_mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nunif_mins_df |&gt;\n  summarise(mean_samp_dist = mean(unif_mins),\n            var_samp_dist = var(unif_mins),\n            sd_samp_dist = sd(unif_mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           8.00         0.694        0.833\n\n## Uniform Sampling Distribution of the Sample Maximum\n\ngenerate_samp_max_unif &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nunif_maxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_unif(theta1 = theta1, theta2 = theta2, n = n))\n\nunif_maxs_df &lt;- tibble(unif_maxs)\n\nggplot(data = unif_maxs_df, aes(x = unif_maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nunif_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(unif_maxs),\n            var_samp_dist = var(unif_maxs),\n            sd_samp_dist = sd(unif_maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           12.0         0.711        0.843\n\n## Exponential Sampling Distribution of the Sample Minimum\nlambda = 0.5\n\ngenerate_samp_min_exp &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nexp_mins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_exp(lambda = lambda, n = n))\n\nexp_mins_df &lt;- tibble(exp_mins)\n\nggplot(data = exp_mins_df, aes(x = exp_mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nexp_mins_df |&gt;\n  summarise(mean_samp_dist = mean(exp_mins),\n            var_samp_dist = var(exp_mins),\n            sd_samp_dist = sd(exp_mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          0.399         0.159        0.398\n\n## Exponential Sampling Distribution of the Sample Maximum\n\ngenerate_samp_max_exp &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nexp_maxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_exp(lambda = lambda, n = n))\n\nexp_maxs_df &lt;- tibble(exp_maxs)\n\nggplot(data = exp_maxs_df, aes(x = exp_maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nexp_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(exp_maxs),\n            var_samp_dist = var(exp_maxs),\n            sd_samp_dist = sd(exp_maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           4.56          5.91         2.43\n\n## Beta Sampling Distribution of the Sample Minimum\nalpha = 8\nbeta = 2\n\ngenerate_samp_min_beta &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nbeta_mins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_beta(alpha = alpha, beta = beta, n = n))\n\nbeta_mins_df &lt;- tibble(beta_mins)\n\nggplot(data = beta_mins_df, aes(x = beta_mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nbeta_mins_df |&gt;\n  summarise(mean_samp_dist = mean(beta_mins),\n            var_samp_dist = var(beta_mins),\n            sd_samp_dist = sd(beta_mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          0.646        0.0112        0.106\n\n## Beta Sampling Distribution of the Sample Maximum\n\ngenerate_samp_max_beta &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nbeta_maxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_beta(alpha = alpha, beta = beta, n = n))\n\nbeta_maxs_df &lt;- tibble(beta_maxs)\n\nggplot(data = beta_maxs_df, aes(x = beta_maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample mins\",\n       title = paste(\"Sampling Distribution of the \\nSample min when n =\", n))\n\n\n\n\n\n\n\nbeta_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(beta_maxs),\n            var_samp_dist = var(beta_maxs),\n            sd_samp_dist = sd(beta_maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          0.922       0.00209       0.0457\n\n\n\nmin_plot &lt;- bind_rows(norm_mins_df, unif_mins_df, exp_mins_df, beta_mins_df) |&gt;\n  mutate(dist = case_when(\n    !is.na(norm_mins) ~ \"normal(10, 4)\",\n    !is.na(unif_mins) ~ \"uniform(7, 13)\",\n    !is.na(exp_mins) ~ \"exponential(0.5)\",\n    !is.na(beta_mins) ~ \"beta(8, 2)\")) |&gt;\n  mutate(dist = fct_relevel(dist, c(\"normal(10, 4)\", \"uniform(7, 13)\", \n                                    \"exponential(0.5)\", \"beta(8, 2)\")),\n         min = pmin(norm_mins, unif_mins, exp_mins, beta_mins, na.rm = TRUE))\n\nmax_plot &lt;- bind_rows(norm_maxs_df, unif_maxs_df, exp_maxs_df, beta_maxs_df) |&gt;\n  mutate(dist = case_when(\n    !is.na(norm_maxs) ~ \"normal(10, 4)\",\n    !is.na(unif_maxs) ~ \"uniform(7, 13)\",\n    !is.na(exp_maxs) ~ \"exponential(0.5)\",\n    !is.na(beta_maxs) ~ \"beta(8, 2)\")) |&gt;\n  mutate(dist = fct_relevel(dist, c(\"normal(10, 4)\", \"uniform(7, 13)\", \n                                    \"exponential(0.5)\", \"beta(8, 2)\")),\n         max = pmin(norm_maxs, unif_maxs, exp_maxs, beta_maxs, na.rm = TRUE))\n\nggplot(data = pop_plot, aes(x = x, y = dens)) +\n  geom_line() +\n  theme_minimal() +\n  facet_wrap(~ pop, nrow = 1, scales = \"free\") +\n  labs(title = \"Population Distributions for Each Simulation Setting\")\n\n\n\n\n\n\n\nggplot(data = min_plot, aes(x = min)) +\n  geom_histogram(color = \"black\", fill = \"darkred\") +\n  theme_minimal() +\n  facet_wrap(~ dist, nrow = 1, scales = \"free\") +\n  labs(title = \"Sampling Distribution of the Sample Minimum for Each Simulation Setting\")\n\n\n\n\n\n\n\nggplot(data = max_plot, aes(x = max)) +\n  geom_histogram(color = \"black\", fill = \"darkgreen\") +\n  theme_minimal() +\n  facet_wrap(~ dist, nrow = 1, scales = \"free\") +\n  labs(title = \"Sampling Distribution of the Sample Maximum for Each Simulation Setting\")\n\n\n\n\n\n\n\n\n\nTable of Results\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.69\n8.01\n0.398\n0.646\n\n\n\\(\\text{E}(Y_{max})\\)\n12.3\n12.0\n4.57\n0.924\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.32\n0.855\n0.402\n0.107\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.32\n0.848\n2.41\n0.0454\n\n\n\n\n(1) Based on the table of results above, \\(SE(Y_{min}) \\approx SE(Y_{max})\\) for the normal and uniform distributions and not for the exponential and beta distributions. This may be because the normal and uniform distributions are symmetric distributions whereas the exponential and beta distributions are not symmetric.\n\n(2) Calculations for \\(Exp(\\lambda = 0.5), n = 5\\).\nPDF and CDF for \\(Exp(\\lambda = 0.5)\\):\n\\(f(y) = 0.5e^{-0.5y}, y &gt; 0\\)\n\\(F(y) = 1-e^{-0.5y}, y &gt; 0 \\space \\text{since} \\space \\int_0^y 0.5e^{-0.5y}dy = 1-e^{-0.5y}\\)\n\nPDF for \\(Y_{min}\\):\n\\(f_{min}(y) = n(1-F(y))^{n-1}f(y)\\)\n\\(f_{min}(y) = 5(1-(1-e^{-0.5y}))^4(0.5e^{-0.5y})\\) simplifies to…\n\\(f_{min}(y) = 2.5e^{-2.5y}, y &gt; 0\\)\n\nn &lt;- 5\ny &lt;- seq(0, 3, length.out = 1000)\n\n## NOTE: this is not a proper density (it does not integrate to 1).\ndensity &lt;- 2.5 * exp(-(2.5) * y)\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(y, density)\nggplot(data = samp_min_df, aes(x = y, y = density)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"PDF for Exponential(0.5), Ymin\")\n\n\n\n\n\n\n\n\n\nPDF for \\(Y_{max}\\):\n\\(f_{max}(y) = n(F(y))^{n-1}f(y)\\)\n\\(f_{max}(y) = 5(1-e^{-0.5y})^4(0.5e^{-0.5y})\\)\n\nn &lt;- 5\ny &lt;- seq(0, 15, length.out = 1000)\n\n## NOTE: this is not a proper density (it does not integrate to 1).\ndensity &lt;- 5 * (1 - exp(-(0.5) * y)) ^ 4 * (0.5*exp(-(0.5)*y))\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(y, density)\nggplot(data = samp_min_df, aes(x = y, y = density)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"PDF for Exponential(0.5), Ymax\")\n\n\n\n\n\n\n\n\n\nExpected Value and Standard Error for \\(Y_{min}\\):\n\\(E(Y_{min}) = \\int_0^{\\infty}y(2.5e^{-2.5y})dy = 0.4\\)\n\\(E(Y_{min}^2) = \\int_0^{\\infty}y^2(2.5e^{-2.5y})dy = 0.32\\)\n\\(Var(Y_{min}) = E(Y_{min}^2) - E(Y_{min})^2 = 0.32 - 0.4^2 = 0.16\\)\n\\(SE(Y_{min}) = \\sqrt{Var(Y_{min})} = \\sqrt{0.16} = 0.4\\)\nExpected Value and Standard Error for \\(Y_{max}\\):\n\\(E(Y_{max}) = \\int_0^{\\infty}5y(1-e^{-0.5y})^4(0.5e^{-0.5y})dy = 4.57\\)\n\\(E(Y_{max}^2) = \\int_0^{\\infty}5y^2(1-e^{-0.5y})^4(0.5e^{-0.5y})dy = 26.7089\\)\n\\(Var(Y_{max}) = E(Y_{max}^2) - E(Y_{max})^2 = 26.7089 - 4.57^2 = 5.824\\)\n\\(SE(Y_{max}) = \\sqrt{Var(Y_{max})} = \\sqrt{5.824} = 2.41\\)\n\nBrief Conclusion: These theoretical values for expectation and standard error are very similar to the simulated values for the exponential(0.5) distribution. This is reassuring because the theory backs up the results we drew from the simulation."
  },
  {
    "objectID": "posts/02-estimation/index.html",
    "href": "posts/02-estimation/index.html",
    "title": "Mini Project 2: A Meaningful Story About Estimation",
    "section": "",
    "text": "Tennis: A Game of Estimation\nOne word that comes to me when I think of the game of tennis: perfection. Why do I say that? Maybe it’s because I grew up idolizing Roger Federer and fall closely in line with many of his fans who think he had the ‘peRFect’ game. Maybe it’s because the game itself is an active allegory for life. Or maybe it’s because it’s just you and your opponent out there, creating one of the most pristine competitive situations the world of sports will ever see. What does the victor have over the loser of the contest? Is it skill? Endurance? Passion? Or is it something more quantitative that we can attribute to the basis of statistical estimation? Although all of these do certainly play a role in determining the winner of a tennis contest, my task is to provide some insight into why tennis is a game of estimation.\nLet’s start with the most basic parameter a tennis player has to pay attention to: where is my opponent going to hit the ball. At the base of the game, if you know where your opponent is going to hit the ball, you already have a significant leg up against your opponent. Many think that this part of tennis is a guessing game, but in reality (at the highest levels of the sport), it most definitely is not. These top professional players can hit the ball so hard with so much accuracy and spin, that guessing where the ball is going to go will be a death sentence on court. These top players – take Novak Djokovic for example – need to come up with an accurate estimator for where the ball will go even before their opponent makes contact with the ball.\nDo you ever wonder why Djokovic makes playing at the highest level of the sport look so easy? It’s because he has mastered and fine-tuned this estimator to give him the most accurate estimate of where the ball is going. It all comes down to two main random variables: court positioning of the opponent and what their racket take-back looks like.\nLet’s dive into court positioning first. Given a random sample of, say, 1000 shots, looking at court positioning as the random variable. The majority of those 1000 shots will go cross-court, based on the geometry of the court (there is the most room for error cross-court since the distance across the court is longer than the distance directly down the line) and the height of the net (which is lowest at the middle of the court). For this random sample, let’s say there are 750 cross-court shots. We could compute the likelihood of the ball getting hit cross-court, but in order to maximize the likelihood, we should also consider the opponent could hit the ball down the line the 250 other times. By just using court positioning, our estimate for where the ball will go has a bias towards the cross-court shot. This is where the opponent’s racket take-back comes into play.\nThere are two parts of the opponent’s racket take-back: racket-face direction and torso coil. The direction the racket-face is pointing can tell you (most of the time) what type of spin will be on the ball when the opponent is hitting it which, depending on the opponent, can provide information about where the ball will be hit. If the racket-face is open (facing up), the incoming shot will either be a slice or a drop-shot; if the racket-face is closed (facing down), the incoming shot will have topspin. At this point, you might ask: how is the type of spin going to help me estimate where my opponent is going to hit the ball? My simple answer is: it doesn’t… by itself.\nBut it can if you consider how your opponent is coiling their torso. Reality check: now we’re getting into the nitty gritty of high-level tennis – keep in mind the best players in the world even have trouble doing this every single shot because it requires such a high degree of focus to do so. Now back into the world of tennis theory: the torso coil and foot positioning also play a role in where the opponent is going to hit the ball. If your opponent is more turned into their take-back, the chances of them hitting the ball down the line are higher.\nThe racket-face and torso coil can’t really tell you where your opponent is going to hit the ball if they’re good at disguising it – this is where a high degree of variance can come into play, but tennis is a complex sport so you can’t really expect there to be a perfectly consistent estimator for shot location (even with a sample size as large as Rafael Nadal at the French Open). However, combining those two factors and the opponent’s court positioning into one all-encompassing estimator, you can get a very good estimate for where they will hit the ball to.\nWith all that said, it’s impossible to think about all those random variables every time you’re preparing to hit the ball. It turns out that for the best tennis players in the world, they’ve played the game so many times that “reading” where the opponent is going to hit the ball has just become second nature. With Novak Djokovic, for example, it seems like he knows where the ball is going even before his opponent knows. Can this heightened level of premonition be attributed to Djokovic’s genius mind being able to compute the maximum likelihood estimator of shot location in a matter of milliseconds or has it just become second nature for him through his countless hours of training and competing at the highest level of tennis for almost 20 years? Or maybe these two things are one in the same."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Statistics Portfolio",
    "section": "",
    "text": "Reflection\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\nBrody Pinto\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 5: Advantages and Drawbacks of Using p-values\n\n\n\n\n\n\n\n\nApr 18, 2025\n\n\nBrody Pinto\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 4: Bayesian Analysis\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nBrody Pinto\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 3: Simulation to Investigate Confidence Intervals\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nBrody Pinto and Lily Kendall\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2: A Meaningful Story About Estimation\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nBrody Pinto\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 1: Sampling Distribution of the Sample Minimum and Maximum\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\nBrody Pinto\n\n\n\n\n\nNo matching items"
  }
]